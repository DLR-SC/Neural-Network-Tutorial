{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a 2-input neuron:\n",
    "![alt text](https://github.com/DLR-SC/Neural-Network-Tutorial/raw/master/images/2Input_neruon.png)\n",
    "\n",
    "    \n",
    "with weights $\\pmb{w}=[w_1,w_2]$ and bias $b$ and a sigmoid activation function $f =1 / (1 + e^{-x})$.<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a quick calculation example, we will set $w = [0,1]$ and $b=4$.\n",
    "Now the output for an input $[x_1, x_2] = [2,3]$ is calculated as follows:\n",
    "<ol>\n",
    "<li><p> Calculate weighted sum, adding the bias (in vector notation using the dot product) \n",
    "<p>    $\\Sigma = \\pmb{w}\\cdot \\pmb{x} + b = (x_1 * w_1)+(x_2 * w_2) + b \\\\\n",
    "            = 0*2 + 1*3 + 4\\\\\n",
    "            = 7$</li>\n",
    "<li><p> Pass the weighted sum through the activation function\n",
    "    <p> $y = f(\\Sigma)\\\\\n",
    "       = f(7) = 0.999$</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing this simple *perceptron* in python (using numpy) is fairly simple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  # Activation function\n",
    "  return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, weights, bias):\n",
    "    self.weights = weights\n",
    "    self.bias = bias\n",
    "\n",
    "  def feedforward(self, inputs):\n",
    "    # Weight inputs, add bias, then use the activation function\n",
    "    total = np.dot(self.weights, inputs) + self.bias\n",
    "    return sigmoid(total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the example from above weights = , bias =,  x = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of running an input through the neurons in a network is known as __feed forward__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network\n",
    "Now lets move to more complexity and add *hidden layers*, to build a complete neural network.  \n",
    "A simple network with an input layer with 2 input neurons, 1 hidden layer with 2 neurons ($\\pmb{h}=[h_1,h_2]$) and an outputlayer with one neuron  might look like this:\n",
    "![alt text](https://github.com/DLR-SC/Neural-Network-Tutorial/raw/master/images/SimpleNetwork.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go back to our previous example and asume that all neurons have again the same weights $\\pmb{w^{(0)}}=\\pmb{w^{(1)}} = [0, 1] $ and the biases are zero ($b_1 = b_2 = b_3 = 0$), running our previous input $\\pmb{x}=[2,3]$ through the network is straight forward:   \n",
    "<br>\n",
    "$h_1 = h_2 = f(\\pmb{w}\\cdot \\pmb{x}+b)\\\\\n",
    "= f((0*2)+(1*3)+0)\\\\\n",
    "= f(3)\\\\\n",
    "=0.9526$</br>\n",
    "\n",
    "$o_1 = f(\\pmb{w}\\cdot \\pmb{h}+b)\\\\\n",
    "= f((0*h_1)+(1*h_2)+0\n",
    "= f(0.95266)\\\\\n",
    "=0.7216$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleNeuralNetwork:\n",
    "  def __init__(self):\n",
    "    weights = np.array([0, 1])\n",
    "    bias = 0\n",
    "\n",
    "    self.h1 = Neuron(weights, bias)\n",
    "    self.h2 = Neuron(weights, bias)\n",
    "    self.o1 = Neuron(weights, bias)\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    out_h1 = self.h1.feedforward(x)\n",
    "    out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "    return out_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run feed forward on x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "Evaluation of the outcome $y_{pred}$ of a network is done via the __Lossfunction__. There are many ways to define the loss, one is via the *mean square error*  \n",
    "$$ MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_{true} - y_{pred})^2$$\n",
    "withn $n$ being the number of samples and $y$ the variable to be predicted (i.e. the outcome of the network).\n",
    "The goal in training a neural network is _minimizing the loss_ by adjusting the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length.\n",
    "    return ((y_true - y_pred) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network\n",
    "\n",
    "Assume we have the following measurements, where the output vector [male, female] is represented by [0,1].\n",
    "\n",
    "| Name   | Weight [kg ] | Height [cm] | Gender |\n",
    "|--------|--------|-------|--------|\n",
    "|Susan   | 51     | 160  |   f = 1   |\n",
    "|Max     | 72     | 178  |   m = 0   |\n",
    "|Lucy    | 68     | 175  |   f = 1   |\n",
    "|Pete    | 71     | 182  |   m = 0   |\n",
    "|Paul    | 90     | 194  |   m = 0   | \n",
    "\n",
    "Given that data, we want to predict someone’s gender given their weight and height. In order to do so, we'll train the simple 2 input network from above, that has one hidden layer with 2 neurons, and a single output :\n",
    "![alt text](https://github.com/DLR-SC/Neural-Network-Tutorial/raw/master/images/SimpleNetwork2.png)\n",
    "\n",
    "$$h_1 = f(\\pmb{w^{(1)}}\\cdot \\pmb{x}+b_1) = f(w_1*x_1 + w_2*x_2 +b_1)$$\n",
    "\n",
    "$$h_2 = f(\\pmb{w^{(2)}}\\cdot \\pmb{x}+b_2) = f(w_3*x_1 + w_4*x_2 +b_2)$$\n",
    "\n",
    "$$y_{pred} = o = f(\\pmb{w^{(h)}}\\cdot \\pmb{h} +b_3) = f(w_5*h_1+w_6 *h_2 +b_3)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork:\n",
    "  def __init__(self, weights1,weights2, weightsh, bias1,bias2, bias3):\n",
    "    \n",
    "    self.h1 = Neuron(weights1, bias1)\n",
    "    self.h2 = Neuron(weights2, bias2)\n",
    "    self.o1 = Neuron(weightsh, bias3)\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    out_h1 = self.h1.feedforward(x)\n",
    "    out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "    return out_o1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example let's have a look at a loss if the network predicts all participants are male. Then the network outputs 0 for all gender predictions and the loss yields:  \n",
    "$$MSE = \\frac{1}{5} ((1-0)^2 + (0-0)^2 + (1-0)^2 + (0-0)^2 + (0-0)^2) = \\frac{2}{5} = 0.4 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's calculate the loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "Now let's look at the example above and set all the weights to 1 and all the biases to 0, i.e. \n",
    "$$\\pmb{w^{(1)}}=\\pmb{w^{(2)}}= \\pmb{w^{(h)}}= [1, 1]\\quad b_1=b_2=b_3 = 0$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = weights2 = weightsh = np.array([1, 1])\n",
    "bias1 = bias2 = bias3 = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that we have just one dataset in our network, since this simplifies things for illustration of the next steps. For easier calculation, the data is shifted by subtraction of $60 kg$ from the weight and $170 cm$ from the height:  \n",
    "\n",
    "| Name   | Weight [kg]<br>(minus 60)  | Height [cm]<br>(minus 170)| Gender |\n",
    "|--------|--------|-------|--------|\n",
    "|Susan | -9     | -10  |   1    |\n",
    "\n",
    "Feed-forward of for example the input \"Susan\"  $x=[-9,-10]$ through the network yields:\n",
    "$$h_1 = f(\\pmb{w^{(1)}}\\cdot \\pmb{x}+b_1), \\: h_2 = f(\\pmb{w^{(2)}}\\cdot \\pmb{x}+b_2)$$\n",
    "\n",
    "$$h_1 = h_2 = f(1*(-9) + 1*(-10) +0)= f(-19) = 5.603*10^{-9}$$\n",
    "\n",
    "$$y_{pred} = o = f(\\pmb{w^{(h)}}\\cdot \\pmb{h} +b_3) = f(w_5*h_1+w_6 *h_2 +b_3)\\\\\n",
    "= f(1*5.603 *10^{-9} + 1*5.603*10^{-9} +0)\\approx 0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward susan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize the loss, we look at it in terms of a multivariate function of $w$ and $b$: \n",
    "$$L (w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3)$$  \n",
    "<br>\n",
    "\n",
    "\n",
    "Then the loss with respect to a certain predicted value is just:\n",
    "$$L = \\frac{1}{1} \\sum_{i=1}^{1} (y_{true} - y_{pred})^2 = (y_{true} - y_{pred})^2 = (1- y_{pred})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we wanted to tweak $w_1$. How would loss $L$ change if we changed $w_1$? \n",
    "In order to see the effects of changing $w_1$ on the loss, we look at the partial derivative\n",
    "$ \\frac{\\partial L}{\\partial w_1} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the loss $L$ is directly influenced by $y_{pred}$, which in term is is determined by $h_1$ and $h_2$. \n",
    "Looking at our network definition above, we see that that $w_1$ only affects the first term of $h_1$.\n",
    "Thus, we can write the partial derivative of $L$ with respect to $w_1$ as:\n",
    "$$ \\frac{\\partial L}{\\partial w_1} =  \\frac{\\partial L}{\\partial y_{pred}} \\cdot\\frac{\\partial y_{pred}}{\\partial h_1}\\cdot \\frac{\\partial h_1}{\\partial w_1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative with respect to $y_{pred}$ is then fairly easy:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y_{pred}} = -2*(1-y_{pred})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dLdy (ypred):\n",
    "    return -2*(1-ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second derivative, we get:\n",
    "$$\\begin{aligned} y_{pred}&= f(w_5*h_1+w_6 *h_2 +b_3) \\\\\n",
    "\\rightarrow \\quad \\frac{\\partial y_{pred}}{\\partial h_1} &= f'(w_5*h_1+w_6 *h_2 +b_3) * w_5 = f'(\\pmb{w^{(h)}}\\cdot \\pmb{h} +b_3)*w^{(h)}(1)\n",
    "\\end{aligned}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dydh1 (h, weightsh, b3):\n",
    "    return d_sigmoid(np.dot(h, weightsh)+b3)*weightsh[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step we get\n",
    "$$\\begin{aligned}\n",
    "h_1 &=  f(w_1*x_1 + w_2*x_2 +b_1)\\\\\n",
    "\\rightarrow \\quad \\frac{\\partial h_1}{\\partial w_1} &= f'(w_1*x_1 + w_2*x_2 +b_1)* x_1= f'(\\pmb{w^{(1)}}\\cdot \\pmb{x}+b_1)*x(1)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the derivative of the sigmoid function is:\n",
    "\n",
    "$$ f'(x) = \\frac{d}{dx}\\left( \\frac{1}{1+e^{-x}}\\right) = \\frac{e^{-x}}{(1-e^{-x})^2} = f(x) *(1-f(x))$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dh1dw1(x, weights1, b1):\n",
    "    return d_sigmoid(np.dot(x, weights1)+b1)*x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid (x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to implement this and calculate the derivative of loss on the above example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this might get very cumbersome, as we want to keep track of all the intermediate steps of derivatives and all the calculated values. And we only looked at the influence of one weight!\n",
    "For large networks, with thousands of nodes on multiple hidden layers with tens of thousands corresponding weights and biases, this quickly gets computationally challenging, also in terms of data storage. \n",
    "However, there is a neat trick to work around this problem:\n",
    "Going back a step, the first thing we can do after feed-forward calculation of $y_{pred}$ of a certain input is to calculate the derivative of the loss with respect to this result, i.e. how much does $L$ depend on $y_{pred}$:\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial y_{pred}} &= -2*(1-y_{pred})\\\\ \n",
    "&= -2* (1- 0.5)\\\\ &= -0.999 \n",
    "\\end{aligned}$$\n",
    "\n",
    "Now $y_{pred}$ itself depends on $h_1$ and $h_2$, as we saw before. And we can easily calculate, _how much_ $y_{pred}$ depends on these two, i.e. the derivatives: \n",
    "$$\\begin{aligned}\n",
    "y_{pred}&= f(w_5*h_1+w_6 *h_2 +b_3) \\\\\n",
    "h_1 = h_2 = f(-19) = 5.603 e-09 \\\\\n",
    "\\frac{\\partial y_{pred}}{\\partial h_1} &= f(w_5*h_1+w_6 *h_2 +b_3)* (1- f(w_5*h_1+w_6 *h_2 +b_3))* w_5 \\\\\n",
    "&= f(1*5.603 e-09  + 1* 5.603 e-09  +0) * (1-f(1*5.603 e-09  + 1* 5.603 e-09  +0)*1\\\\ \n",
    "&= 0.25\\\\\n",
    "\\frac{\\partial y_{pred}}{\\partial h_2} &= f(w_5*h_1+w_6 *h_2 +b_3)* (1- f(w_5*h_1+w_6 *h_2 +b_3))* w_6 \\\\\n",
    "&= 0.25\n",
    "\\end{aligned}$$\n",
    "\n",
    "If we combine these two results so far, we end up with:\n",
    "$$\\begin{aligned}\\frac{\\partial L}{\\partial h_1} &= \\frac{\\partial L}{\\partial y_{pred}} \\cdot\\frac{\\partial y_{pred}}{\\partial h_1} = -0.999 * 0.25 = -0.2499 \\\\\n",
    "\\frac{\\partial L}{\\partial h_2} &= \\frac{\\partial L}{\\partial y_{pred}} \\cdot\\frac{\\partial y_{pred}}{\\partial h_2} = -0.999 * 0.25 = -0.2499 \n",
    "\\end{aligned}$$\n",
    "<br>\n",
    "At this point, we have the direct dependency of $L$ on the hidden layer nodes $h_1,h_2$, so we don't need to remember the derivatives with respect to $y_{pred}$ anymore.... already a save in memory!\n",
    "\n",
    "In the same manner, the dependency of $h_1$ on $w_1$ can be calcluated as\n",
    "$$\\begin{aligned}\\frac{\\partial h_1}{\\partial w_1} &= f(w_1*x_1 + w_2*x_2 +b_1)* (1-f(w_1*x_1 + w_2*x_2 +b_1) )*x_1\\\\\n",
    "&= f(1*-9+1*-10+0)*(1-f(1*-9+1*-10+0))*1 = f(-19)*(1+f(-19))*1 \\\\\n",
    "&= -5.0425*10^{-8}\n",
    "\\end{aligned}$$\n",
    "$h_2$ doesn't depend on $w_1$ at all, so that derivative is just 0.\n",
    "\n",
    "So the overall derivative of $L$ with respect to $w_1$ now yields:\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial w_1} &= \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial w_1}\\\\ \n",
    "&= -0.2499 * -5.0425*10^{-8} \\\\\n",
    "&= 1.2606*10^{-8}\n",
    "\\end{aligned}$$\n",
    "\n",
    "This result means that if we were to increase $w_1$, $L$ would increase a tiny bit as a result.  \n",
    "The system of calculating partial derivatives by working backwards is known as __backpropagation__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Now we have all the tools we need to train the network. Training means gradually increasing/decreasing the weights and biases, in order to minimize the loss function. Minimization is part of an _optimization problem_.<br>\n",
    "For that we’ll use an optimization algorithm called _gradient descent_, that changes the weights and biases according to their impact on the loss function, i.e.:\n",
    "$$ w_1 \\rightarrow w_1 - \\eta \\frac{\\partial L}{\\partial w_1}$$\n",
    "- If ${\\partial L}/{\\partial w_1}$ is positive, $w_1$ will decrease, which makes $L$ decrease.\n",
    "- If ${\\partial L}/{\\partial w_1}$ is negative, $w_1$ will increase, which makes $L$ decrease.  \n",
    "\n",
    "\n",
    "$\\eta$ is called the learning rate and controls how fast we train.\n",
    "\n",
    "Of course we don't do this updating process not only for $w_1$, but for all the weights and biases in the network,  which will slowly decrease the loss and improve the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the network on the data set we had before:\n",
    "\n",
    "| Name   | Weight [kg]<br>(minus 60)  | Height [cm]<br>(minus 170)| Gender |\n",
    "|--------|--------|-------|--------|\n",
    "|Susan | -9     | -10  |   1   |\n",
    "|Max  | 12     | 8  |   0   |\n",
    "|Lucy    | 8      | 5  |   1   |\n",
    "|Pete    | 11     | 12  |   0   |\n",
    "|Paul   | 30     | 24  |   0   |  \n",
    "\n",
    "The training process is as follows:\n",
    "1. Choose one sample from our dataset. This is what makes it gradient descent - we only operate on one sample at a time.\n",
    "2. Calculate all the partial derivatives of loss with respect to weights or biases (e.g. ${\\partial L}/{\\partial w_1}$, ${\\partial L}/{\\partial w_2}$, etc).\n",
    "3. Use the update equation to update each weight and bias.\n",
    "4. Repeat untill loss converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Weights\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Biases\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "        #For ilusstration\n",
    "        self.loss = np.zeros((2,1000))\n",
    "\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # x is a numpy array with 2 elements.\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "              for x, y_true in zip(data, all_y_trues):\n",
    "                # --- Do a feedforward (we'll need these values later)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # --- Calculate partial derivatives.\n",
    "                # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * d_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * d_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = d_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h1 = self.w5 * d_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * d_sigmoid(sum_o1)\n",
    "\n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * d_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * d_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = d_sigmoid(sum_h1)\n",
    "\n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * d_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * d_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = d_sigmoid(sum_h2)\n",
    "\n",
    "                # --- Update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Neuron h2\n",
    "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Neuron o1\n",
    "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "              # --- Calculate total loss at the end of each epoch\n",
    "              self.loss[:,epoch] = [epoch, mse_loss(all_y_trues, np.apply_along_axis(self.feedforward, 1, data))]\n",
    "\n",
    "              if epoch % 50 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                print(\"Epoch %d loss: %.3f\" % (epoch, loss))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "# Height - 170, weight - 60 (data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our neural network!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the loss is slowly decreasing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(network2.loss[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our network does in predicting the gender of two unknown data points:\n",
    "\n",
    "| Name   | Weight (kg) | Hight (cm) | Gender |\n",
    "|--------|--------|-------|--------|\n",
    "|Mary | 9      | 0  |   f = 1   |\n",
    "|John     | 10     | -3  |   m = 0   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for Mary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for John"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on a whole grid for all possible combinations in 45 - 95 kg, 150 - 195 cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male and female training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm... What could have gone wrong?\n",
    " - too few data points for training \n",
    "   more free parameters in network () than data points\n",
    "   --> Overfitting\n",
    " - model too complex \n",
    "   --> linear regression\n",
    " - Features not sufficient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shown implementation of the neural network is just meant to be _illustrative_, to show how the network learns. Of course this is not a proper implementation, designed for efficiency. Especially the part on the derivatives is suboptimal, as you would naturally never implement out all these derivative equations explicitely by hand.... imagine having a network with hundreds of nodes in several input layers!  \n",
    "For such tasks, frameworks like _autograd_ are very handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layered neural networks\n",
    "So far our examples have been fairly simple: we used only 6 weights (four to connect the input layer with the hidden layer and 2 to connect the hidden layer with the output layer) and thus simply could enumerate them.\n",
    "However, in large neural networks, where all neurons of one layer are in priniciple connected with all neurons of the following layer, a _matrix notation_ is much handier.\n",
    "\n",
    "![alt text](https://github.com/DLR-SC/Neural-Network-Tutorial/raw/master/images/Network_Math.png)\n",
    "We will denote the weight conneting the $i^{th}$ node of the $n^{th}$ layer to the $j^{th}$ node of the $(n+1)^{th}$ layer with $w_{ij}^{(n)}$. The corresponding bias is denoted as $b_{j}^{(n)}$. With this, the activations $h^{(n+1)}$ in the $(n+1)^{th}$ layer can be stated in a matrix equation:\n",
    "$$\n",
    "h^{(n+1)} = W^{(n)}\\cdot h^{(n)} + \\pmb{b^{(n)}} \\quad with\\quad W^{(n)}=\\begin{bmatrix}\n",
    "  w_{00}^{(n)} & ... & w_{0j}^{(n)} & ... \\\\\n",
    "  ... & .. & .. &.. \\\\\n",
    "  w_{i0}^{(n)}& ... & w_{ij}^{(n)} & ... \\\\\n",
    "  ... & .. & .. &.. \\\\\n",
    " \\end{bmatrix}\n",
    " \\quad and \\quad \\pmb{b^{(n)}}= \\begin{pmatrix} b_0^{(n)} \\\\ .. \\\\ b_j^{(n)}\\\\ .. \\end{pmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References ##\n",
    "*The calculation example with code snippets are adapted from:* <br>\n",
    "https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9\n",
    "\n",
    "__Further reading:__<br>\n",
    "Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.  \n",
    "Bishop, Christopher M. Neural networks for pattern recognition. Oxford university press, 1995.  \n",
    "<br>\n",
    "https://skymind.ai/wiki/neural-network  \n",
    "https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/  \n",
    "http://neuralnetworksanddeeplearning.com/chap1.html  \n",
    "https://www.youtube.com/watch?v=aircAruvnKk  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
